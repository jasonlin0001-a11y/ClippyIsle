import Foundation
import AVFoundation
import Speech
import MediaPlayer
import Combine // å¿…é ˆåŠ å…¥ï¼Œä¿®æ­£ ObservableObject éŒ¯èª¤
import SwiftUI // å»ºè­°åŠ å…¥ï¼Œä¿®æ­£æ½›åœ¨çš„ UI ç›¸é—œå¼•ç”¨

// MARK: - Audio Manager
class AudioManager {
    static let shared = AudioManager()
    private var lastScenario: AudioScenario?

    private init() {}

    enum AudioScenario {
        case speechPlayback
        case webViewPlayback
        case speechRecognition
        case idle
    }

    func setup(for scenario: AudioScenario) {
        let session = AVAudioSession.sharedInstance()
        do {
            if lastScenario == .webViewPlayback && scenario == .speechPlayback {
                try session.setActive(false, options: .notifyOthersOnDeactivation)
            }
            
            switch scenario {
            case .speechPlayback:
                try session.setCategory(.playback, mode: .spokenAudio, options: [.allowBluetoothA2DP, .allowAirPlay])
            case .webViewPlayback:
                try session.setCategory(.playback, mode: .moviePlayback, options: [])
            case .speechRecognition:
                try session.setCategory(.record, mode: .measurement, options: .duckOthers)
            case .idle:
                try session.setActive(false, options: .notifyOthersOnDeactivation)
                lastScenario = nil
                return
            }
            try session.setActive(true)
            lastScenario = scenario
        } catch {
            print("âŒ AudioManager: ç‚º \(scenario) è¨­å®š audio session å¤±æ•—ã€‚éŒ¯èª¤: \(error.localizedDescription)")
        }
    }

    func deactivate() {
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
            MPNowPlayingInfoCenter.default().nowPlayingInfo = nil
            lastScenario = nil
            print("âœ… AudioManager: Audio session å·²åœç”¨ä¸” NowPlayingInfo å·²æ¸…é™¤ã€‚")
        } catch {
            print("âŒ AudioManager: åœç”¨ audio session å¤±æ•—ã€‚éŒ¯èª¤: \(error.localizedDescription)")
        }
    }
}

// MARK: - Speech Recognizer
class SpeechRecognizer: ObservableObject {
    @Published var transcript: String = ""
    private var speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()

    init() {
        LaunchLogger.log("SpeechRecognizer.init() - START requesting authorization")
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                if authStatus != .authorized { 
                    print("âŒ èªéŸ³è¾¨è­˜æ¬Šé™æœªæˆæ¬Šã€‚")
                    LaunchLogger.log("SpeechRecognizer.init() - Authorization DENIED")
                }
                else { 
                    print("âœ… èªéŸ³è¾¨è­˜æ¬Šé™å·²æˆæ¬Šã€‚")
                    LaunchLogger.log("SpeechRecognizer.init() - Authorization GRANTED")
                }
            }
        }
        LaunchLogger.log("SpeechRecognizer.init() - END (async authorization request sent)")
    }
    
    func startTranscribing() {
        guard !audioEngine.isRunning else { stopTranscribing(); return }
        guard SFSpeechRecognizer.authorizationStatus() == .authorized else {
            print("âŒ ç„¡æ³•é–‹å§‹ï¼ŒèªéŸ³è¾¨è­˜æ¬Šé™æœªæˆæ¬Šã€‚")
            return
        }
        
        do {
            AudioManager.shared.setup(for: .speechRecognition)
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest else { fatalError("ç„¡æ³•å»ºç«‹è¾¨è­˜è«‹æ±‚") }
            recognitionRequest.shouldReportPartialResults = true

            let inputNode = audioEngine.inputNode
            recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                if let result {
                    DispatchQueue.main.async { self?.transcript = result.bestTranscription.formattedString }
                }
                if error != nil { self?.stopTranscribing() }
            }

            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
                self.recognitionRequest?.append(buffer)
            }

            audioEngine.prepare()
            try audioEngine.start()
            print("ğŸ™ï¸ èªéŸ³è¾¨è­˜å·²å•Ÿå‹•...")

        } catch {
            print("âŒ èªéŸ³è¾¨è­˜å•Ÿå‹•å¤±æ•—: \(error)")
            stopTranscribing()
        }
    }

    func stopTranscribing() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        
        AudioManager.shared.deactivate()
        print("ğŸ›‘ èªéŸ³è¾¨è­˜å·²åœæ­¢ã€‚")
    }
}